{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pathogenic?</th>\n",
       "      <th>history_class</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>cancer_dx</th>\n",
       "      <th>known_brca</th>\n",
       "      <th>cancer_dx_type_1</th>\n",
       "      <th>cancer_dx_age_1</th>\n",
       "      <th>cancer_dx_type_2</th>\n",
       "      <th>cancer_dx_age_2</th>\n",
       "      <th>cancer_dx_type_3</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_23_cancer_4</th>\n",
       "      <th>rel_23_cancer_5</th>\n",
       "      <th>rel_23_cancer_6</th>\n",
       "      <th>rel_23_cancer_7</th>\n",
       "      <th>rel_23_cancer_8</th>\n",
       "      <th>rel_23_cancer_9</th>\n",
       "      <th>rel_23_cancer_10</th>\n",
       "      <th>rel_23_age_1</th>\n",
       "      <th>rel_23_age_2</th>\n",
       "      <th>rel_23_age_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46669</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46670</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46672 rows Ã— 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pathogenic?  history_class  ethnicity  cancer_dx  known_brca  \\\n",
       "0                0              3          6          1          -1   \n",
       "1                0              3          6          0          -1   \n",
       "2                0              3          2          1          -1   \n",
       "3                0              3          6          1          -1   \n",
       "4                0              3          6          1          -1   \n",
       "...            ...            ...        ...        ...         ...   \n",
       "46667            0              0          6          0          -1   \n",
       "46668            0              0          6          0          -1   \n",
       "46669            0              0          5          1          -1   \n",
       "46670            0              0          6          0           0   \n",
       "46671            0              0          0          0          -1   \n",
       "\n",
       "       cancer_dx_type_1  cancer_dx_age_1  cancer_dx_type_2  cancer_dx_age_2  \\\n",
       "0                    57               57                 0               -1   \n",
       "1                    32               32                 0               -1   \n",
       "2                    12               37                 0               -1   \n",
       "3                    40               55                 0               -1   \n",
       "4                    55               58                 0               -1   \n",
       "...                 ...              ...               ...              ...   \n",
       "46667                 0               -1                 0               -1   \n",
       "46668                 0               -1                 0               -1   \n",
       "46669                13               22                 0               -1   \n",
       "46670                 0               -1                 0               -1   \n",
       "46671                 0               -1                 0               -1   \n",
       "\n",
       "       cancer_dx_type_3  ...  rel_23_cancer_4  rel_23_cancer_5  \\\n",
       "0                     0  ...                0                0   \n",
       "1                     0  ...                0                0   \n",
       "2                     0  ...                0                0   \n",
       "3                     0  ...                0                0   \n",
       "4                     0  ...                0                0   \n",
       "...                 ...  ...              ...              ...   \n",
       "46667                 0  ...                0                0   \n",
       "46668                 0  ...                0                0   \n",
       "46669                 0  ...                0                0   \n",
       "46670                 0  ...                0                0   \n",
       "46671                 0  ...                0                0   \n",
       "\n",
       "       rel_23_cancer_6  rel_23_cancer_7  rel_23_cancer_8  rel_23_cancer_9  \\\n",
       "0                    0                0                0                0   \n",
       "1                    0                0                0                0   \n",
       "2                    0                0                0                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "46667                0                0                0                0   \n",
       "46668                0                0                0                0   \n",
       "46669                0                0                0                0   \n",
       "46670                0                0                0                0   \n",
       "46671                0                0                0                0   \n",
       "\n",
       "       rel_23_cancer_10  rel_23_age_1  rel_23_age_2  rel_23_age_3  \n",
       "0                     0            -1            -1            -1  \n",
       "1                     0            -1            -1            -1  \n",
       "2                     0            -1            -1            -1  \n",
       "3                     0            -1            -1            -1  \n",
       "4                     0            -1            -1            -1  \n",
       "...                 ...           ...           ...           ...  \n",
       "46667                 0            -1            -1            -1  \n",
       "46668                 0            81            -1            -1  \n",
       "46669                 0            -1            -1            -1  \n",
       "46670                 0            -1            -1            -1  \n",
       "46671                 0            39            -1            -1  \n",
       "\n",
       "[46672 rows x 333 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "df = pd.read_csv('./data/reaugmented_3.csv')\n",
    "\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.iloc[2213:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df2.loc[:, df.columns != 'history_class']  # Features\n",
    "y=df2['history_class']  # Labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df.loc[:, df.columns != 'history_class']  # Features\n",
    "y=df['history_class']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16192, 332)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_test = torch.LongTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 256, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim = 332, output_dim = 4):\n",
    "        super(ANN, self).__init__()\n",
    "    \n",
    "        # Input Layer (784) -> 784\n",
    "        self.fc1 = nn.Linear(input_dim, 784)\n",
    "        # 784 -> 128\n",
    "        self.fc2 = nn.Linear(784, 128)\n",
    "        # 128 -> 128\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        # 128 -> 64\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        # 64 -> 64\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        # 64 -> 32\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        # 32 -> 32\n",
    "        self.fc7 = nn.Linear(32, 32)\n",
    "        # 32 -> output layer(10)\n",
    "        self.output_layer = nn.Linear(32,4)\n",
    "        # Dropout Layer (20%) to reduce overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    # Feed Forward Function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten image input\n",
    "        \n",
    "        # Add ReLU activation function to each layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        # Return the created model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(input_dim = 332, output_dim = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.8987\t Val. acc: 59.40%\n",
      "Validation loss decreased (inf --> 0.826148).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.7757\t Val. acc: 64.17%\n",
      "Validation loss decreased (0.826148 --> 0.739015).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.7121\t Val. acc: 65.95%\n",
      "Validation loss decreased (0.739015 --> 0.708334).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.6759\t Val. acc: 67.54%\n",
      "Validation loss decreased (0.708334 --> 0.660147).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.6358\t Val. acc: 68.35%\n",
      "Validation loss decreased (0.660147 --> 0.602628).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.6116\t Val. acc: 69.14%\n",
      "Epoch: 7 \tTraining Loss: 0.5960\t Val. acc: 71.48%\n",
      "Epoch: 8 \tTraining Loss: 0.5858\t Val. acc: 71.48%\n",
      "Validation loss decreased (0.602628 --> 0.595873).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.5593\t Val. acc: 73.68%\n",
      "Validation loss decreased (0.595873 --> 0.545190).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.5559\t Val. acc: 72.11%\n",
      "Epoch: 11 \tTraining Loss: 0.5359\t Val. acc: 73.80%\n",
      "Epoch: 12 \tTraining Loss: 0.5136\t Val. acc: 77.46%\n",
      "Validation loss decreased (0.545190 --> 0.490025).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.4999\t Val. acc: 74.36%\n",
      "Epoch: 14 \tTraining Loss: 0.4989\t Val. acc: 77.87%\n",
      "Validation loss decreased (0.490025 --> 0.486803).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.4735\t Val. acc: 79.19%\n",
      "Validation loss decreased (0.486803 --> 0.452014).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.4691\t Val. acc: 79.67%\n",
      "Validation loss decreased (0.452014 --> 0.451197).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.4591\t Val. acc: 79.49%\n",
      "Epoch: 18 \tTraining Loss: 0.4486\t Val. acc: 80.79%\n",
      "Validation loss decreased (0.451197 --> 0.443117).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.4323\t Val. acc: 80.81%\n",
      "Validation loss decreased (0.443117 --> 0.424045).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.4168\t Val. acc: 80.95%\n",
      "Validation loss decreased (0.424045 --> 0.420563).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.4271\t Val. acc: 82.93%\n",
      "Validation loss decreased (0.420563 --> 0.413127).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.4112\t Val. acc: 86.28%\n",
      "Validation loss decreased (0.413127 --> 0.371580).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.3942\t Val. acc: 83.11%\n",
      "Epoch: 24 \tTraining Loss: 0.3915\t Val. acc: 83.06%\n",
      "Epoch: 25 \tTraining Loss: 0.3895\t Val. acc: 82.46%\n",
      "Epoch: 26 \tTraining Loss: 0.3847\t Val. acc: 83.20%\n",
      "Epoch: 27 \tTraining Loss: 0.3708\t Val. acc: 81.80%\n",
      "Epoch: 28 \tTraining Loss: 0.3740\t Val. acc: 85.74%\n",
      "Validation loss decreased (0.371580 --> 0.362750).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.3662\t Val. acc: 84.68%\n",
      "Epoch: 30 \tTraining Loss: 0.3688\t Val. acc: 87.31%\n",
      "Validation loss decreased (0.362750 --> 0.339888).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.3527\t Val. acc: 86.50%\n",
      "Epoch: 32 \tTraining Loss: 0.3491\t Val. acc: 86.82%\n",
      "Validation loss decreased (0.339888 --> 0.329737).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.3410\t Val. acc: 86.95%\n",
      "Epoch: 34 \tTraining Loss: 0.3383\t Val. acc: 86.32%\n",
      "Validation loss decreased (0.329737 --> 0.327755).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.3436\t Val. acc: 87.34%\n",
      "Epoch: 36 \tTraining Loss: 0.3253\t Val. acc: 87.94%\n",
      "Validation loss decreased (0.327755 --> 0.310787).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.3271\t Val. acc: 88.19%\n",
      "Epoch: 38 \tTraining Loss: 0.3240\t Val. acc: 87.52%\n",
      "Epoch: 39 \tTraining Loss: 0.3312\t Val. acc: 86.77%\n",
      "Epoch: 40 \tTraining Loss: 0.3156\t Val. acc: 86.71%\n",
      "Epoch: 41 \tTraining Loss: 0.3099\t Val. acc: 88.21%\n",
      "Validation loss decreased (0.310787 --> 0.308488).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.3195\t Val. acc: 87.38%\n",
      "Epoch: 43 \tTraining Loss: 0.3256\t Val. acc: 87.83%\n",
      "Epoch: 44 \tTraining Loss: 0.3028\t Val. acc: 88.19%\n",
      "Validation loss decreased (0.308488 --> 0.299204).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.3077\t Val. acc: 88.73%\n",
      "Validation loss decreased (0.299204 --> 0.290892).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.3005\t Val. acc: 86.41%\n",
      "Epoch: 47 \tTraining Loss: 0.3061\t Val. acc: 86.98%\n",
      "Epoch: 48 \tTraining Loss: 0.3016\t Val. acc: 88.21%\n",
      "Epoch: 49 \tTraining Loss: 0.2940\t Val. acc: 88.91%\n",
      "Validation loss decreased (0.290892 --> 0.286821).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.2996\t Val. acc: 87.20%\n",
      "Epoch: 51 \tTraining Loss: 0.2919\t Val. acc: 88.55%\n",
      "Epoch: 52 \tTraining Loss: 0.2930\t Val. acc: 88.75%\n",
      "Validation loss decreased (0.286821 --> 0.281626).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.3066\t Val. acc: 88.60%\n",
      "Epoch: 54 \tTraining Loss: 0.2857\t Val. acc: 89.41%\n",
      "Epoch: 55 \tTraining Loss: 0.3031\t Val. acc: 86.86%\n",
      "Epoch: 56 \tTraining Loss: 0.2813\t Val. acc: 90.26%\n",
      "Validation loss decreased (0.281626 --> 0.262920).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.2822\t Val. acc: 88.69%\n",
      "Epoch: 58 \tTraining Loss: 0.2980\t Val. acc: 86.82%\n",
      "Epoch: 59 \tTraining Loss: 0.2955\t Val. acc: 89.38%\n",
      "Epoch: 60 \tTraining Loss: 0.2823\t Val. acc: 86.57%\n",
      "Epoch: 61 \tTraining Loss: 0.2791\t Val. acc: 88.12%\n",
      "Epoch: 62 \tTraining Loss: 0.2836\t Val. acc: 89.68%\n",
      "Epoch: 63 \tTraining Loss: 0.2848\t Val. acc: 90.06%\n",
      "Epoch: 64 \tTraining Loss: 0.2824\t Val. acc: 90.28%\n",
      "Epoch: 65 \tTraining Loss: 0.2741\t Val. acc: 89.65%\n",
      "Epoch: 66 \tTraining Loss: 0.2807\t Val. acc: 88.91%\n",
      "Epoch: 67 \tTraining Loss: 0.2796\t Val. acc: 88.12%\n",
      "Epoch: 68 \tTraining Loss: 0.2826\t Val. acc: 88.78%\n",
      "Epoch: 69 \tTraining Loss: 0.2783\t Val. acc: 87.63%\n",
      "Epoch: 70 \tTraining Loss: 0.2773\t Val. acc: 89.50%\n",
      "Epoch: 71 \tTraining Loss: 0.2787\t Val. acc: 87.52%\n",
      "Epoch: 72 \tTraining Loss: 0.2732\t Val. acc: 90.73%\n",
      "Validation loss decreased (0.262920 --> 0.258066).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.2621\t Val. acc: 90.17%\n",
      "Validation loss decreased (0.258066 --> 0.255865).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.2679\t Val. acc: 90.42%\n",
      "Epoch: 75 \tTraining Loss: 0.2774\t Val. acc: 90.24%\n",
      "Epoch: 76 \tTraining Loss: 0.2632\t Val. acc: 89.27%\n",
      "Epoch: 77 \tTraining Loss: 0.2631\t Val. acc: 89.86%\n",
      "Epoch: 78 \tTraining Loss: 0.2733\t Val. acc: 90.15%\n",
      "Epoch: 79 \tTraining Loss: 0.2619\t Val. acc: 89.38%\n",
      "Epoch: 80 \tTraining Loss: 0.2646\t Val. acc: 89.16%\n",
      "Epoch: 81 \tTraining Loss: 0.2834\t Val. acc: 90.13%\n",
      "Epoch: 82 \tTraining Loss: 0.2670\t Val. acc: 90.10%\n",
      "Epoch: 83 \tTraining Loss: 0.2562\t Val. acc: 89.25%\n",
      "Epoch: 84 \tTraining Loss: 0.2545\t Val. acc: 90.64%\n",
      "Epoch: 85 \tTraining Loss: 0.2590\t Val. acc: 90.51%\n",
      "Epoch: 86 \tTraining Loss: 0.2730\t Val. acc: 90.10%\n",
      "Epoch: 87 \tTraining Loss: 0.2558\t Val. acc: 91.14%\n",
      "Validation loss decreased (0.255865 --> 0.240198).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.2528\t Val. acc: 90.26%\n",
      "Epoch: 89 \tTraining Loss: 0.2570\t Val. acc: 90.37%\n",
      "Epoch: 90 \tTraining Loss: 0.2565\t Val. acc: 90.37%\n",
      "Epoch: 91 \tTraining Loss: 0.2553\t Val. acc: 89.70%\n",
      "Epoch: 92 \tTraining Loss: 0.2512\t Val. acc: 91.34%\n",
      "Epoch: 93 \tTraining Loss: 0.2584\t Val. acc: 91.00%\n",
      "Epoch: 94 \tTraining Loss: 0.2476\t Val. acc: 91.39%\n",
      "Validation loss decreased (0.240198 --> 0.239828).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.2512\t Val. acc: 90.40%\n",
      "Epoch: 96 \tTraining Loss: 0.2757\t Val. acc: 91.05%\n",
      "Epoch: 97 \tTraining Loss: 0.2479\t Val. acc: 86.82%\n",
      "Epoch: 98 \tTraining Loss: 0.2697\t Val. acc: 89.65%\n",
      "Epoch: 99 \tTraining Loss: 0.2651\t Val. acc: 91.16%\n",
      "Epoch: 100 \tTraining Loss: 0.2429\t Val. acc: 89.65%\n",
      "Epoch: 101 \tTraining Loss: 0.2525\t Val. acc: 91.39%\n",
      "Validation loss decreased (0.239828 --> 0.238435).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.2483\t Val. acc: 90.24%\n",
      "Epoch: 103 \tTraining Loss: 0.2500\t Val. acc: 88.87%\n",
      "Epoch: 104 \tTraining Loss: 0.2747\t Val. acc: 91.23%\n",
      "Epoch: 105 \tTraining Loss: 0.2408\t Val. acc: 91.86%\n",
      "Validation loss decreased (0.238435 --> 0.233929).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.2461\t Val. acc: 88.71%\n",
      "Epoch: 107 \tTraining Loss: 0.2412\t Val. acc: 91.59%\n",
      "Epoch: 108 \tTraining Loss: 0.2446\t Val. acc: 91.36%\n",
      "Epoch: 109 \tTraining Loss: 0.2397\t Val. acc: 90.62%\n",
      "Epoch: 110 \tTraining Loss: 0.2383\t Val. acc: 91.81%\n",
      "Validation loss decreased (0.233929 --> 0.231461).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 0.2401\t Val. acc: 89.95%\n",
      "Epoch: 112 \tTraining Loss: 0.2376\t Val. acc: 91.75%\n",
      "Epoch: 113 \tTraining Loss: 0.2478\t Val. acc: 90.78%\n",
      "Epoch: 114 \tTraining Loss: 0.2423\t Val. acc: 91.36%\n",
      "Epoch: 115 \tTraining Loss: 0.2406\t Val. acc: 91.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 \tTraining Loss: 0.2471\t Val. acc: 91.27%\n",
      "Epoch: 117 \tTraining Loss: 0.2423\t Val. acc: 91.84%\n",
      "Validation loss decreased (0.231461 --> 0.228858).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.2360\t Val. acc: 91.27%\n",
      "Epoch: 119 \tTraining Loss: 0.2504\t Val. acc: 92.11%\n",
      "Validation loss decreased (0.228858 --> 0.223991).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 0.2332\t Val. acc: 88.55%\n",
      "Epoch: 121 \tTraining Loss: 0.2327\t Val. acc: 91.86%\n",
      "Epoch: 122 \tTraining Loss: 0.2428\t Val. acc: 90.55%\n",
      "Epoch: 123 \tTraining Loss: 0.2435\t Val. acc: 92.11%\n",
      "Validation loss decreased (0.223991 --> 0.222957).  Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 0.2402\t Val. acc: 91.57%\n",
      "Epoch: 125 \tTraining Loss: 0.2280\t Val. acc: 91.30%\n",
      "Epoch: 126 \tTraining Loss: 0.2214\t Val. acc: 90.60%\n",
      "Epoch: 127 \tTraining Loss: 0.2314\t Val. acc: 92.04%\n",
      "Epoch: 128 \tTraining Loss: 0.2286\t Val. acc: 91.52%\n",
      "Epoch: 129 \tTraining Loss: 0.2234\t Val. acc: 91.41%\n",
      "Epoch: 130 \tTraining Loss: 0.2258\t Val. acc: 91.39%\n",
      "Epoch: 131 \tTraining Loss: 0.2378\t Val. acc: 91.12%\n",
      "Epoch: 132 \tTraining Loss: 0.2337\t Val. acc: 92.06%\n",
      "Epoch: 133 \tTraining Loss: 0.2314\t Val. acc: 91.59%\n",
      "Epoch: 134 \tTraining Loss: 0.2241\t Val. acc: 88.30%\n",
      "Epoch: 135 \tTraining Loss: 0.2335\t Val. acc: 89.95%\n",
      "Epoch: 136 \tTraining Loss: 0.2359\t Val. acc: 90.42%\n",
      "Epoch: 137 \tTraining Loss: 0.2377\t Val. acc: 91.75%\n",
      "Epoch: 138 \tTraining Loss: 0.2487\t Val. acc: 91.66%\n",
      "Epoch: 139 \tTraining Loss: 0.2406\t Val. acc: 91.43%\n",
      "Epoch: 140 \tTraining Loss: 0.2219\t Val. acc: 90.94%\n",
      "Epoch: 141 \tTraining Loss: 0.2212\t Val. acc: 89.83%\n",
      "Epoch: 142 \tTraining Loss: 0.2263\t Val. acc: 91.79%\n",
      "Epoch: 143 \tTraining Loss: 0.2255\t Val. acc: 91.57%\n",
      "Epoch: 144 \tTraining Loss: 0.2352\t Val. acc: 89.97%\n",
      "Epoch: 145 \tTraining Loss: 0.2217\t Val. acc: 91.30%\n",
      "Epoch: 146 \tTraining Loss: 0.2229\t Val. acc: 92.13%\n",
      "Validation loss decreased (0.222957 --> 0.219084).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 0.2366\t Val. acc: 91.70%\n",
      "Epoch: 148 \tTraining Loss: 0.2345\t Val. acc: 92.22%\n",
      "Epoch: 149 \tTraining Loss: 0.2226\t Val. acc: 90.67%\n",
      "Epoch: 150 \tTraining Loss: 0.2237\t Val. acc: 91.63%\n",
      "Epoch: 151 \tTraining Loss: 0.2270\t Val. acc: 91.16%\n",
      "Epoch: 152 \tTraining Loss: 0.2247\t Val. acc: 92.17%\n",
      "Validation loss decreased (0.219084 --> 0.218826).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 0.2247\t Val. acc: 91.32%\n",
      "Epoch: 154 \tTraining Loss: 0.2310\t Val. acc: 91.93%\n",
      "Epoch: 155 \tTraining Loss: 0.2249\t Val. acc: 91.68%\n",
      "Epoch: 156 \tTraining Loss: 0.2214\t Val. acc: 92.08%\n",
      "Epoch: 157 \tTraining Loss: 0.2135\t Val. acc: 91.84%\n",
      "Epoch: 158 \tTraining Loss: 0.2258\t Val. acc: 92.11%\n",
      "Epoch: 159 \tTraining Loss: 0.2345\t Val. acc: 90.53%\n",
      "Epoch: 160 \tTraining Loss: 0.2251\t Val. acc: 91.90%\n",
      "Epoch: 161 \tTraining Loss: 0.2127\t Val. acc: 92.33%\n",
      "Epoch: 162 \tTraining Loss: 0.2131\t Val. acc: 91.32%\n",
      "Epoch: 163 \tTraining Loss: 0.2424\t Val. acc: 90.49%\n",
      "Epoch: 164 \tTraining Loss: 0.2185\t Val. acc: 92.08%\n",
      "Epoch: 165 \tTraining Loss: 0.2189\t Val. acc: 90.73%\n",
      "Epoch: 166 \tTraining Loss: 0.2356\t Val. acc: 90.73%\n",
      "Epoch: 167 \tTraining Loss: 0.2156\t Val. acc: 92.40%\n",
      "Epoch: 168 \tTraining Loss: 0.2240\t Val. acc: 92.29%\n",
      "Epoch: 169 \tTraining Loss: 0.2146\t Val. acc: 91.61%\n",
      "Epoch: 170 \tTraining Loss: 0.2228\t Val. acc: 91.63%\n",
      "Epoch: 171 \tTraining Loss: 0.2152\t Val. acc: 91.90%\n",
      "Epoch: 172 \tTraining Loss: 0.2173\t Val. acc: 91.90%\n",
      "Epoch: 173 \tTraining Loss: 0.2394\t Val. acc: 92.96%\n",
      "Epoch: 174 \tTraining Loss: 0.2189\t Val. acc: 92.31%\n",
      "Validation loss decreased (0.218826 --> 0.215352).  Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 0.2135\t Val. acc: 91.30%\n",
      "Epoch: 176 \tTraining Loss: 0.2336\t Val. acc: 92.60%\n",
      "Validation loss decreased (0.215352 --> 0.215171).  Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 0.2146\t Val. acc: 92.35%\n",
      "Epoch: 178 \tTraining Loss: 0.2066\t Val. acc: 89.27%\n",
      "Epoch: 179 \tTraining Loss: 0.2146\t Val. acc: 92.11%\n",
      "Epoch: 180 \tTraining Loss: 0.2155\t Val. acc: 92.47%\n",
      "Epoch: 181 \tTraining Loss: 0.2178\t Val. acc: 90.82%\n",
      "Epoch: 182 \tTraining Loss: 0.2157\t Val. acc: 91.88%\n",
      "Epoch: 183 \tTraining Loss: 0.2157\t Val. acc: 92.29%\n",
      "Epoch: 184 \tTraining Loss: 0.2131\t Val. acc: 90.98%\n",
      "Epoch: 185 \tTraining Loss: 0.2202\t Val. acc: 91.86%\n",
      "Epoch: 186 \tTraining Loss: 0.2037\t Val. acc: 92.04%\n",
      "Epoch: 187 \tTraining Loss: 0.2102\t Val. acc: 92.49%\n",
      "Epoch: 188 \tTraining Loss: 0.2177\t Val. acc: 91.23%\n",
      "Epoch: 189 \tTraining Loss: 0.2166\t Val. acc: 92.22%\n",
      "Epoch: 190 \tTraining Loss: 0.2136\t Val. acc: 92.60%\n",
      "Epoch: 191 \tTraining Loss: 0.2115\t Val. acc: 91.81%\n",
      "Epoch: 192 \tTraining Loss: 0.2076\t Val. acc: 91.99%\n",
      "Epoch: 193 \tTraining Loss: 0.2095\t Val. acc: 92.31%\n",
      "Epoch: 194 \tTraining Loss: 0.2122\t Val. acc: 92.02%\n",
      "Epoch: 195 \tTraining Loss: 0.2047\t Val. acc: 89.70%\n",
      "Epoch: 196 \tTraining Loss: 0.2187\t Val. acc: 91.39%\n",
      "Epoch: 197 \tTraining Loss: 0.2157\t Val. acc: 92.17%\n",
      "Epoch: 198 \tTraining Loss: 0.2150\t Val. acc: 92.17%\n",
      "Epoch: 199 \tTraining Loss: 0.2159\t Val. acc: 92.71%\n",
      "Validation loss decreased (0.215171 --> 0.214204).  Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 0.2049\t Val. acc: 91.90%\n",
      "Epoch: 201 \tTraining Loss: 0.2090\t Val. acc: 91.54%\n",
      "Epoch: 202 \tTraining Loss: 0.2121\t Val. acc: 90.44%\n",
      "Epoch: 203 \tTraining Loss: 0.2114\t Val. acc: 92.08%\n",
      "Epoch: 204 \tTraining Loss: 0.2085\t Val. acc: 92.94%\n",
      "Epoch: 205 \tTraining Loss: 0.1998\t Val. acc: 92.98%\n",
      "Validation loss decreased (0.214204 --> 0.210762).  Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 0.2024\t Val. acc: 92.06%\n",
      "Epoch: 207 \tTraining Loss: 0.2259\t Val. acc: 92.65%\n",
      "Epoch: 208 \tTraining Loss: 0.2016\t Val. acc: 92.13%\n",
      "Epoch: 209 \tTraining Loss: 0.2090\t Val. acc: 92.87%\n",
      "Epoch: 210 \tTraining Loss: 0.2115\t Val. acc: 91.52%\n",
      "Epoch: 211 \tTraining Loss: 0.2183\t Val. acc: 93.00%\n",
      "Epoch: 212 \tTraining Loss: 0.1998\t Val. acc: 92.02%\n",
      "Epoch: 213 \tTraining Loss: 0.2121\t Val. acc: 92.60%\n",
      "Epoch: 214 \tTraining Loss: 0.2003\t Val. acc: 92.53%\n",
      "Epoch: 215 \tTraining Loss: 0.2078\t Val. acc: 91.54%\n",
      "Epoch: 216 \tTraining Loss: 0.1989\t Val. acc: 92.98%\n",
      "Epoch: 217 \tTraining Loss: 0.2092\t Val. acc: 90.78%\n",
      "Epoch: 218 \tTraining Loss: 0.2092\t Val. acc: 92.98%\n",
      "Validation loss decreased (0.210762 --> 0.205679).  Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 0.1981\t Val. acc: 91.77%\n",
      "Epoch: 220 \tTraining Loss: 0.2088\t Val. acc: 91.93%\n",
      "Epoch: 221 \tTraining Loss: 0.2024\t Val. acc: 92.89%\n",
      "Epoch: 222 \tTraining Loss: 0.1983\t Val. acc: 90.40%\n",
      "Epoch: 223 \tTraining Loss: 0.2069\t Val. acc: 93.21%\n",
      "Epoch: 224 \tTraining Loss: 0.1989\t Val. acc: 92.22%\n",
      "Epoch: 225 \tTraining Loss: 0.2022\t Val. acc: 92.56%\n",
      "Epoch: 226 \tTraining Loss: 0.2009\t Val. acc: 91.93%\n",
      "Epoch: 227 \tTraining Loss: 0.2018\t Val. acc: 92.24%\n",
      "Epoch: 228 \tTraining Loss: 0.2132\t Val. acc: 92.62%\n",
      "Epoch: 229 \tTraining Loss: 0.2173\t Val. acc: 92.49%\n",
      "Epoch: 230 \tTraining Loss: 0.2025\t Val. acc: 91.43%\n",
      "Epoch: 231 \tTraining Loss: 0.2003\t Val. acc: 92.98%\n",
      "Epoch: 232 \tTraining Loss: 0.2030\t Val. acc: 92.17%\n",
      "Epoch: 233 \tTraining Loss: 0.1986\t Val. acc: 92.80%\n",
      "Epoch: 234 \tTraining Loss: 0.2028\t Val. acc: 91.95%\n",
      "Epoch: 235 \tTraining Loss: 0.2020\t Val. acc: 91.95%\n",
      "Epoch: 236 \tTraining Loss: 0.2155\t Val. acc: 92.91%\n",
      "Epoch: 237 \tTraining Loss: 0.2085\t Val. acc: 92.31%\n",
      "Epoch: 238 \tTraining Loss: 0.1962\t Val. acc: 91.99%\n",
      "Epoch: 239 \tTraining Loss: 0.2046\t Val. acc: 91.12%\n",
      "Epoch: 240 \tTraining Loss: 0.2114\t Val. acc: 93.03%\n",
      "Epoch: 241 \tTraining Loss: 0.1940\t Val. acc: 93.48%\n",
      "Epoch: 242 \tTraining Loss: 0.1967\t Val. acc: 93.03%\n",
      "Epoch: 243 \tTraining Loss: 0.1968\t Val. acc: 92.24%\n",
      "Epoch: 244 \tTraining Loss: 0.1969\t Val. acc: 92.69%\n",
      "Epoch: 245 \tTraining Loss: 0.2128\t Val. acc: 93.36%\n",
      "Epoch: 246 \tTraining Loss: 0.1981\t Val. acc: 92.33%\n",
      "Epoch: 247 \tTraining Loss: 0.1972\t Val. acc: 92.85%\n",
      "Epoch: 248 \tTraining Loss: 0.1984\t Val. acc: 92.87%\n",
      "Epoch: 249 \tTraining Loss: 0.2006\t Val. acc: 93.43%\n",
      "Epoch: 250 \tTraining Loss: 0.1948\t Val. acc: 92.69%\n",
      "Epoch: 251 \tTraining Loss: 0.1984\t Val. acc: 93.27%\n",
      "Epoch: 252 \tTraining Loss: 0.2090\t Val. acc: 93.34%\n",
      "Validation loss decreased (0.205679 --> 0.204712).  Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 0.1837\t Val. acc: 93.12%\n",
      "Epoch: 254 \tTraining Loss: 0.2027\t Val. acc: 92.74%\n",
      "Epoch: 255 \tTraining Loss: 0.1994\t Val. acc: 93.00%\n",
      "Epoch: 256 \tTraining Loss: 0.1978\t Val. acc: 92.49%\n",
      "Epoch: 257 \tTraining Loss: 0.1983\t Val. acc: 92.44%\n",
      "Epoch: 258 \tTraining Loss: 0.2029\t Val. acc: 93.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259 \tTraining Loss: 0.1973\t Val. acc: 91.66%\n",
      "Epoch: 260 \tTraining Loss: 0.1982\t Val. acc: 90.26%\n",
      "Epoch: 261 \tTraining Loss: 0.1977\t Val. acc: 92.94%\n",
      "Epoch: 262 \tTraining Loss: 0.2024\t Val. acc: 93.34%\n",
      "Validation loss decreased (0.204712 --> 0.204118).  Saving model ...\n",
      "Epoch: 263 \tTraining Loss: 0.2057\t Val. acc: 91.88%\n",
      "Epoch: 264 \tTraining Loss: 0.1984\t Val. acc: 92.22%\n",
      "Epoch: 265 \tTraining Loss: 0.1948\t Val. acc: 92.65%\n",
      "Validation loss decreased (0.204118 --> 0.203543).  Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 0.1914\t Val. acc: 92.76%\n",
      "Epoch: 267 \tTraining Loss: 0.2080\t Val. acc: 91.97%\n",
      "Epoch: 268 \tTraining Loss: 0.2014\t Val. acc: 92.65%\n",
      "Epoch: 269 \tTraining Loss: 0.2033\t Val. acc: 89.97%\n",
      "Epoch: 270 \tTraining Loss: 0.2036\t Val. acc: 92.56%\n",
      "Epoch: 271 \tTraining Loss: 0.2137\t Val. acc: 92.94%\n",
      "Epoch: 272 \tTraining Loss: 0.1926\t Val. acc: 93.30%\n",
      "Epoch: 273 \tTraining Loss: 0.1879\t Val. acc: 92.51%\n",
      "Epoch: 274 \tTraining Loss: 0.2163\t Val. acc: 93.12%\n",
      "Validation loss decreased (0.203543 --> 0.199389).  Saving model ...\n",
      "Epoch: 275 \tTraining Loss: 0.1932\t Val. acc: 93.00%\n",
      "Epoch: 276 \tTraining Loss: 0.1901\t Val. acc: 92.65%\n",
      "Epoch: 277 \tTraining Loss: 0.1943\t Val. acc: 86.98%\n",
      "Epoch: 278 \tTraining Loss: 0.2287\t Val. acc: 92.42%\n",
      "Epoch: 279 \tTraining Loss: 0.1931\t Val. acc: 92.56%\n",
      "Epoch: 280 \tTraining Loss: 0.1929\t Val. acc: 92.76%\n",
      "Epoch: 281 \tTraining Loss: 0.1945\t Val. acc: 93.32%\n",
      "Validation loss decreased (0.199389 --> 0.198789).  Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 0.2168\t Val. acc: 90.10%\n",
      "Epoch: 283 \tTraining Loss: 0.1959\t Val. acc: 93.03%\n",
      "Epoch: 284 \tTraining Loss: 0.1864\t Val. acc: 92.87%\n",
      "Epoch: 285 \tTraining Loss: 0.1886\t Val. acc: 92.47%\n",
      "Epoch: 286 \tTraining Loss: 0.1936\t Val. acc: 92.94%\n",
      "Epoch: 287 \tTraining Loss: 0.1994\t Val. acc: 93.50%\n",
      "Epoch: 288 \tTraining Loss: 0.1979\t Val. acc: 92.67%\n",
      "Epoch: 289 \tTraining Loss: 0.1933\t Val. acc: 92.89%\n",
      "Epoch: 290 \tTraining Loss: 0.1992\t Val. acc: 92.76%\n",
      "Epoch: 291 \tTraining Loss: 0.1917\t Val. acc: 91.93%\n",
      "Epoch: 292 \tTraining Loss: 0.1942\t Val. acc: 92.91%\n",
      "Epoch: 293 \tTraining Loss: 0.2003\t Val. acc: 90.96%\n",
      "Epoch: 294 \tTraining Loss: 0.1931\t Val. acc: 92.83%\n",
      "Epoch: 295 \tTraining Loss: 0.1886\t Val. acc: 93.34%\n",
      "Epoch: 296 \tTraining Loss: 0.1846\t Val. acc: 93.03%\n",
      "Epoch: 297 \tTraining Loss: 0.2020\t Val. acc: 90.89%\n",
      "Epoch: 298 \tTraining Loss: 0.1938\t Val. acc: 93.16%\n",
      "Epoch: 299 \tTraining Loss: 0.1934\t Val. acc: 92.89%\n",
      "Epoch: 300 \tTraining Loss: 0.1959\t Val. acc: 92.71%\n",
      "Epoch: 301 \tTraining Loss: 0.1871\t Val. acc: 92.42%\n",
      "Epoch: 302 \tTraining Loss: 0.1902\t Val. acc: 92.78%\n",
      "Epoch: 303 \tTraining Loss: 0.2009\t Val. acc: 90.33%\n",
      "Epoch: 304 \tTraining Loss: 0.1993\t Val. acc: 92.62%\n",
      "Epoch: 305 \tTraining Loss: 0.1935\t Val. acc: 93.57%\n",
      "Epoch: 306 \tTraining Loss: 0.1902\t Val. acc: 90.53%\n",
      "Epoch: 307 \tTraining Loss: 0.1945\t Val. acc: 92.76%\n",
      "Epoch: 308 \tTraining Loss: 0.1876\t Val. acc: 92.74%\n",
      "Epoch: 309 \tTraining Loss: 0.1894\t Val. acc: 92.49%\n",
      "Epoch: 310 \tTraining Loss: 0.1975\t Val. acc: 92.56%\n",
      "Epoch: 311 \tTraining Loss: 0.1971\t Val. acc: 90.26%\n",
      "Epoch: 312 \tTraining Loss: 0.2038\t Val. acc: 92.65%\n",
      "Epoch: 313 \tTraining Loss: 0.1904\t Val. acc: 93.39%\n",
      "Epoch: 314 \tTraining Loss: 0.1957\t Val. acc: 92.71%\n",
      "Epoch: 315 \tTraining Loss: 0.1863\t Val. acc: 90.98%\n",
      "Epoch: 316 \tTraining Loss: 0.1959\t Val. acc: 92.11%\n",
      "Epoch: 317 \tTraining Loss: 0.1886\t Val. acc: 92.11%\n",
      "Epoch: 318 \tTraining Loss: 0.1938\t Val. acc: 90.15%\n",
      "Epoch: 319 \tTraining Loss: 0.1861\t Val. acc: 92.26%\n",
      "Epoch: 320 \tTraining Loss: 0.1919\t Val. acc: 92.22%\n",
      "Epoch: 321 \tTraining Loss: 0.1886\t Val. acc: 93.43%\n",
      "Validation loss decreased (0.198789 --> 0.197659).  Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 0.1906\t Val. acc: 92.87%\n",
      "Epoch: 323 \tTraining Loss: 0.1988\t Val. acc: 92.96%\n",
      "Epoch: 324 \tTraining Loss: 0.1919\t Val. acc: 93.30%\n",
      "Validation loss decreased (0.197659 --> 0.196845).  Saving model ...\n",
      "Epoch: 325 \tTraining Loss: 0.1936\t Val. acc: 93.18%\n",
      "Epoch: 326 \tTraining Loss: 0.1833\t Val. acc: 92.60%\n",
      "Epoch: 327 \tTraining Loss: 0.2081\t Val. acc: 92.94%\n",
      "Epoch: 328 \tTraining Loss: 0.1953\t Val. acc: 92.24%\n",
      "Epoch: 329 \tTraining Loss: 0.1933\t Val. acc: 92.40%\n",
      "Epoch: 330 \tTraining Loss: 0.1829\t Val. acc: 93.05%\n",
      "Epoch: 331 \tTraining Loss: 0.1888\t Val. acc: 93.03%\n",
      "Epoch: 332 \tTraining Loss: 0.1954\t Val. acc: 92.60%\n",
      "Epoch: 333 \tTraining Loss: 0.1974\t Val. acc: 92.87%\n",
      "Epoch: 334 \tTraining Loss: 0.1932\t Val. acc: 92.98%\n",
      "Epoch: 335 \tTraining Loss: 0.1855\t Val. acc: 91.90%\n",
      "Epoch: 336 \tTraining Loss: 0.1981\t Val. acc: 93.03%\n",
      "Epoch: 337 \tTraining Loss: 0.1867\t Val. acc: 92.49%\n",
      "Epoch: 338 \tTraining Loss: 0.1937\t Val. acc: 92.40%\n",
      "Epoch: 339 \tTraining Loss: 0.1831\t Val. acc: 93.12%\n",
      "Epoch: 340 \tTraining Loss: 0.1820\t Val. acc: 91.70%\n",
      "Epoch: 341 \tTraining Loss: 0.2021\t Val. acc: 93.14%\n",
      "Epoch: 342 \tTraining Loss: 0.1850\t Val. acc: 92.51%\n",
      "Epoch: 343 \tTraining Loss: 0.1828\t Val. acc: 92.22%\n",
      "Epoch: 344 \tTraining Loss: 0.1926\t Val. acc: 93.72%\n",
      "Validation loss decreased (0.196845 --> 0.193457).  Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 0.1789\t Val. acc: 93.45%\n",
      "Epoch: 346 \tTraining Loss: 0.1967\t Val. acc: 92.15%\n",
      "Epoch: 347 \tTraining Loss: 0.1849\t Val. acc: 93.59%\n",
      "Epoch: 348 \tTraining Loss: 0.1831\t Val. acc: 93.32%\n",
      "Epoch: 349 \tTraining Loss: 0.1872\t Val. acc: 93.32%\n",
      "Epoch: 350 \tTraining Loss: 0.1846\t Val. acc: 93.30%\n",
      "Epoch: 351 \tTraining Loss: 0.2011\t Val. acc: 92.91%\n",
      "Epoch: 352 \tTraining Loss: 0.1778\t Val. acc: 93.50%\n",
      "Epoch: 353 \tTraining Loss: 0.1838\t Val. acc: 93.54%\n",
      "Epoch: 354 \tTraining Loss: 0.1795\t Val. acc: 93.61%\n",
      "Epoch: 355 \tTraining Loss: 0.1906\t Val. acc: 93.36%\n",
      "Epoch: 356 \tTraining Loss: 0.1921\t Val. acc: 92.06%\n",
      "Epoch: 357 \tTraining Loss: 0.1910\t Val. acc: 93.05%\n",
      "Epoch: 358 \tTraining Loss: 0.1833\t Val. acc: 90.78%\n",
      "Epoch: 359 \tTraining Loss: 0.1855\t Val. acc: 92.29%\n",
      "Epoch: 360 \tTraining Loss: 0.1842\t Val. acc: 92.44%\n",
      "Epoch: 361 \tTraining Loss: 0.2020\t Val. acc: 92.35%\n",
      "Epoch: 362 \tTraining Loss: 0.1867\t Val. acc: 93.23%\n",
      "Epoch: 363 \tTraining Loss: 0.1884\t Val. acc: 93.09%\n",
      "Epoch: 364 \tTraining Loss: 0.1795\t Val. acc: 92.91%\n",
      "Epoch: 365 \tTraining Loss: 0.1960\t Val. acc: 92.20%\n",
      "Epoch: 366 \tTraining Loss: 0.1868\t Val. acc: 92.62%\n",
      "Epoch: 367 \tTraining Loss: 0.1861\t Val. acc: 92.58%\n",
      "Epoch: 368 \tTraining Loss: 0.1791\t Val. acc: 92.31%\n",
      "Epoch: 369 \tTraining Loss: 0.1857\t Val. acc: 91.59%\n",
      "Epoch: 370 \tTraining Loss: 0.1832\t Val. acc: 92.38%\n",
      "Epoch: 371 \tTraining Loss: 0.1881\t Val. acc: 92.13%\n",
      "Epoch: 372 \tTraining Loss: 0.1910\t Val. acc: 93.12%\n",
      "Epoch: 373 \tTraining Loss: 0.1842\t Val. acc: 93.12%\n",
      "Epoch: 374 \tTraining Loss: 0.1801\t Val. acc: 91.97%\n",
      "Epoch: 375 \tTraining Loss: 0.1904\t Val. acc: 93.52%\n",
      "Epoch: 376 \tTraining Loss: 0.1801\t Val. acc: 93.23%\n",
      "Epoch: 377 \tTraining Loss: 0.1786\t Val. acc: 93.14%\n",
      "Epoch: 378 \tTraining Loss: 0.1864\t Val. acc: 93.75%\n",
      "Epoch: 379 \tTraining Loss: 0.1810\t Val. acc: 92.56%\n",
      "Epoch: 380 \tTraining Loss: 0.1861\t Val. acc: 93.50%\n",
      "Epoch: 381 \tTraining Loss: 0.1901\t Val. acc: 93.09%\n",
      "Epoch: 382 \tTraining Loss: 0.1900\t Val. acc: 92.62%\n",
      "Epoch: 383 \tTraining Loss: 0.1827\t Val. acc: 91.25%\n",
      "Epoch: 384 \tTraining Loss: 0.2131\t Val. acc: 92.65%\n",
      "Epoch: 385 \tTraining Loss: 0.1820\t Val. acc: 93.14%\n",
      "Epoch: 386 \tTraining Loss: 0.1805\t Val. acc: 92.69%\n",
      "Epoch: 387 \tTraining Loss: 0.1793\t Val. acc: 93.14%\n",
      "Epoch: 388 \tTraining Loss: 0.1783\t Val. acc: 92.94%\n",
      "Epoch: 389 \tTraining Loss: 0.1891\t Val. acc: 92.80%\n",
      "Epoch: 390 \tTraining Loss: 0.1815\t Val. acc: 93.25%\n",
      "Epoch: 391 \tTraining Loss: 0.1838\t Val. acc: 93.03%\n",
      "Epoch: 392 \tTraining Loss: 0.1750\t Val. acc: 93.52%\n",
      "Epoch: 393 \tTraining Loss: 0.1806\t Val. acc: 90.80%\n",
      "Epoch: 394 \tTraining Loss: 0.1835\t Val. acc: 93.41%\n",
      "Epoch: 395 \tTraining Loss: 0.1970\t Val. acc: 90.13%\n",
      "Epoch: 396 \tTraining Loss: 0.1931\t Val. acc: 93.18%\n",
      "Epoch: 397 \tTraining Loss: 0.1839\t Val. acc: 93.14%\n",
      "Epoch: 398 \tTraining Loss: 0.1756\t Val. acc: 93.63%\n",
      "Epoch: 399 \tTraining Loss: 0.1725\t Val. acc: 93.39%\n",
      "Epoch: 400 \tTraining Loss: 0.1764\t Val. acc: 92.76%\n",
      "Epoch: 401 \tTraining Loss: 0.1820\t Val. acc: 93.43%\n",
      "Epoch: 402 \tTraining Loss: 0.1773\t Val. acc: 93.43%\n",
      "Validation loss decreased (0.193457 --> 0.192997).  Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 0.1787\t Val. acc: 93.12%\n",
      "Epoch: 404 \tTraining Loss: 0.1841\t Val. acc: 93.23%\n",
      "Epoch: 405 \tTraining Loss: 0.1768\t Val. acc: 93.16%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 406 \tTraining Loss: 0.1779\t Val. acc: 93.43%\n",
      "Epoch: 407 \tTraining Loss: 0.1770\t Val. acc: 93.39%\n",
      "Epoch: 408 \tTraining Loss: 0.1750\t Val. acc: 93.34%\n",
      "Epoch: 409 \tTraining Loss: 0.1783\t Val. acc: 93.30%\n",
      "Epoch: 410 \tTraining Loss: 0.1757\t Val. acc: 93.66%\n",
      "Epoch: 411 \tTraining Loss: 0.1767\t Val. acc: 92.89%\n",
      "Epoch: 412 \tTraining Loss: 0.1819\t Val. acc: 92.53%\n",
      "Epoch: 413 \tTraining Loss: 0.1830\t Val. acc: 92.69%\n",
      "Epoch: 414 \tTraining Loss: 0.1858\t Val. acc: 89.45%\n",
      "Epoch: 415 \tTraining Loss: 0.1876\t Val. acc: 93.23%\n",
      "Epoch: 416 \tTraining Loss: 0.1720\t Val. acc: 93.25%\n",
      "Epoch: 417 \tTraining Loss: 0.1768\t Val. acc: 93.09%\n",
      "Epoch: 418 \tTraining Loss: 0.1753\t Val. acc: 91.43%\n",
      "Epoch: 419 \tTraining Loss: 0.1855\t Val. acc: 92.85%\n",
      "Epoch: 420 \tTraining Loss: 0.1892\t Val. acc: 92.51%\n",
      "Epoch: 421 \tTraining Loss: 0.1787\t Val. acc: 92.85%\n",
      "Epoch: 422 \tTraining Loss: 0.1755\t Val. acc: 93.52%\n",
      "Epoch: 423 \tTraining Loss: 0.1890\t Val. acc: 92.44%\n",
      "Epoch: 424 \tTraining Loss: 0.1807\t Val. acc: 89.61%\n",
      "Epoch: 425 \tTraining Loss: 0.1884\t Val. acc: 90.51%\n",
      "Epoch: 426 \tTraining Loss: 0.1805\t Val. acc: 93.50%\n",
      "Epoch: 427 \tTraining Loss: 0.1754\t Val. acc: 93.05%\n",
      "Epoch: 428 \tTraining Loss: 0.1710\t Val. acc: 92.31%\n",
      "Epoch: 429 \tTraining Loss: 0.1833\t Val. acc: 93.39%\n",
      "Epoch: 430 \tTraining Loss: 0.1833\t Val. acc: 93.09%\n",
      "Epoch: 431 \tTraining Loss: 0.1832\t Val. acc: 92.71%\n",
      "Epoch: 432 \tTraining Loss: 0.1860\t Val. acc: 92.89%\n",
      "Epoch: 433 \tTraining Loss: 0.1776\t Val. acc: 93.21%\n",
      "Epoch: 434 \tTraining Loss: 0.1728\t Val. acc: 93.00%\n",
      "Epoch: 435 \tTraining Loss: 0.1758\t Val. acc: 93.27%\n",
      "Epoch: 436 \tTraining Loss: 0.1706\t Val. acc: 92.69%\n",
      "Epoch: 437 \tTraining Loss: 0.1850\t Val. acc: 93.14%\n",
      "Epoch: 438 \tTraining Loss: 0.1928\t Val. acc: 91.27%\n",
      "Epoch: 439 \tTraining Loss: 0.1729\t Val. acc: 93.34%\n",
      "Epoch: 440 \tTraining Loss: 0.1751\t Val. acc: 93.00%\n",
      "Epoch: 441 \tTraining Loss: 0.1848\t Val. acc: 93.16%\n",
      "Epoch: 442 \tTraining Loss: 0.1776\t Val. acc: 93.21%\n",
      "Epoch: 443 \tTraining Loss: 0.1690\t Val. acc: 93.68%\n",
      "Epoch: 444 \tTraining Loss: 0.1666\t Val. acc: 93.30%\n",
      "Epoch: 445 \tTraining Loss: 0.1798\t Val. acc: 93.61%\n",
      "Epoch: 446 \tTraining Loss: 0.1774\t Val. acc: 93.07%\n",
      "Epoch: 447 \tTraining Loss: 0.1818\t Val. acc: 93.34%\n",
      "Epoch: 448 \tTraining Loss: 0.1854\t Val. acc: 92.58%\n",
      "Epoch: 449 \tTraining Loss: 0.1914\t Val. acc: 93.00%\n",
      "Epoch: 450 \tTraining Loss: 0.1833\t Val. acc: 92.22%\n",
      "Epoch: 451 \tTraining Loss: 0.1770\t Val. acc: 93.34%\n",
      "Epoch: 452 \tTraining Loss: 0.1723\t Val. acc: 93.57%\n",
      "Epoch: 453 \tTraining Loss: 0.1698\t Val. acc: 92.87%\n",
      "Epoch: 454 \tTraining Loss: 0.1770\t Val. acc: 92.20%\n",
      "Epoch: 455 \tTraining Loss: 0.1725\t Val. acc: 93.30%\n",
      "Epoch: 456 \tTraining Loss: 0.1776\t Val. acc: 92.91%\n",
      "Epoch: 457 \tTraining Loss: 0.1719\t Val. acc: 92.94%\n",
      "Epoch: 458 \tTraining Loss: 0.1787\t Val. acc: 92.85%\n",
      "Epoch: 459 \tTraining Loss: 0.1813\t Val. acc: 93.03%\n",
      "Epoch: 460 \tTraining Loss: 0.1835\t Val. acc: 93.03%\n",
      "Epoch: 461 \tTraining Loss: 0.1860\t Val. acc: 93.39%\n",
      "Epoch: 462 \tTraining Loss: 0.1778\t Val. acc: 92.33%\n",
      "Epoch: 463 \tTraining Loss: 0.1845\t Val. acc: 93.61%\n",
      "Epoch: 464 \tTraining Loss: 0.1783\t Val. acc: 92.85%\n",
      "Epoch: 465 \tTraining Loss: 0.1736\t Val. acc: 93.39%\n",
      "Epoch: 466 \tTraining Loss: 0.1703\t Val. acc: 93.32%\n",
      "Epoch: 467 \tTraining Loss: 0.1764\t Val. acc: 91.72%\n",
      "Epoch: 468 \tTraining Loss: 0.1782\t Val. acc: 93.05%\n",
      "Epoch: 469 \tTraining Loss: 0.1786\t Val. acc: 93.09%\n",
      "Epoch: 470 \tTraining Loss: 0.1783\t Val. acc: 93.05%\n",
      "Epoch: 471 \tTraining Loss: 0.1783\t Val. acc: 93.45%\n",
      "Epoch: 472 \tTraining Loss: 0.1650\t Val. acc: 93.23%\n",
      "Epoch: 473 \tTraining Loss: 0.1760\t Val. acc: 92.51%\n",
      "Epoch: 474 \tTraining Loss: 0.1808\t Val. acc: 93.66%\n",
      "Epoch: 475 \tTraining Loss: 0.1825\t Val. acc: 93.12%\n",
      "Epoch: 476 \tTraining Loss: 0.1755\t Val. acc: 92.89%\n",
      "Epoch: 477 \tTraining Loss: 0.1738\t Val. acc: 91.61%\n",
      "Epoch: 478 \tTraining Loss: 0.1789\t Val. acc: 93.23%\n",
      "Epoch: 479 \tTraining Loss: 0.1861\t Val. acc: 92.26%\n",
      "Epoch: 480 \tTraining Loss: 0.1743\t Val. acc: 93.41%\n",
      "Epoch: 481 \tTraining Loss: 0.1758\t Val. acc: 93.50%\n",
      "Epoch: 482 \tTraining Loss: 0.1748\t Val. acc: 93.39%\n",
      "Epoch: 483 \tTraining Loss: 0.1721\t Val. acc: 93.03%\n",
      "Epoch: 484 \tTraining Loss: 0.1692\t Val. acc: 92.47%\n",
      "Epoch: 485 \tTraining Loss: 0.1752\t Val. acc: 91.57%\n",
      "Epoch: 486 \tTraining Loss: 0.1804\t Val. acc: 92.91%\n",
      "Epoch: 487 \tTraining Loss: 0.1713\t Val. acc: 92.87%\n",
      "Epoch: 488 \tTraining Loss: 0.1800\t Val. acc: 93.43%\n",
      "Epoch: 489 \tTraining Loss: 0.1867\t Val. acc: 93.12%\n",
      "Epoch: 490 \tTraining Loss: 0.1688\t Val. acc: 93.00%\n",
      "Epoch: 491 \tTraining Loss: 0.1747\t Val. acc: 93.54%\n",
      "Epoch: 492 \tTraining Loss: 0.1729\t Val. acc: 93.14%\n",
      "Epoch: 493 \tTraining Loss: 0.1784\t Val. acc: 92.91%\n",
      "Epoch: 494 \tTraining Loss: 0.1747\t Val. acc: 92.91%\n",
      "Epoch: 495 \tTraining Loss: 0.1765\t Val. acc: 93.05%\n",
      "Epoch: 496 \tTraining Loss: 0.1791\t Val. acc: 93.18%\n",
      "Epoch: 497 \tTraining Loss: 0.1730\t Val. acc: 93.12%\n",
      "Epoch: 498 \tTraining Loss: 0.1752\t Val. acc: 93.05%\n",
      "Epoch: 499 \tTraining Loss: 0.1803\t Val. acc: 93.63%\n",
      "Epoch: 500 \tTraining Loss: 0.1742\t Val. acc: 93.16%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Start epochs\n",
    "for epoch in range(epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    # Set the training mode ON -> Activate Dropout Layers\n",
    "    model.train() # prepare model for training\n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Load Train Images with Labels(Targets)\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Convert our images and labels to Variables to accumulate Gradients\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.LongTensor)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate Training Accuracy \n",
    "        predicted = torch.max(output.data, 1)[1]        \n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = loss_fn(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average training loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    # Avg Accuracy\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    train_acc_list.append(accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "        \n",
    "    # Implement Validation like K-fold Cross-validation \n",
    "    # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "    model.eval() # Required for Evaluation/Test\n",
    "\n",
    "    # Calculate Test/Validation Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            # Convert our images and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # Predict Output\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "    \n",
    "    # calculate average training loss and accuracy over an epoch\n",
    "    val_loss = val_loss/len(test_loader.dataset)\n",
    "    accuracy = 100 * correct/ float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    val_acc_list.append(accuracy)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # Print the Epoch and Training Loss Details with Validation Accuracy   \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        accuracy\n",
    "        ))\n",
    "    # save model if validation loss has decreased\n",
    "    if val_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        val_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = val_loss\n",
    "    # Move to next epoch\n",
    "    epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.loc[0:2212,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=df3.loc[:, df.columns != 'history_class']  # Features\n",
    "y2=df3['history_class']  # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = torch.FloatTensor(X2.values)\n",
    "y_test2 = torch.LongTensor(y2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(X_test2)):\n",
    "    if y_test2[0]==torch.argmax(model(X_test2[i])):\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022593764121102574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/(len(X_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regular",
   "language": "python",
   "name": "regular"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
